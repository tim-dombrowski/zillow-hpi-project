---
title: "Zillow House Value Index Comparisons"
output: html_notebook
---

## R Packages

* The [devtools package](https://cran.r-project.org/package=devtools) contains the `install_github()` function, which will allow us to install the urbnmapr package from GitHub.
* The [urbnmapr package](https://urban-institute.medium.com/how-to-create-state-and-county-maps-easily-in-r-577d29300bb2) has mapping data for various geographies.
* The [readr package](https://cran.r-project.org/package=readr) is a common package for reading in data files. After installing, the RStudio Environment tab will have a new option to import data sets that uses this package. It will even generate the R code for you to copy and paste into your script.
* The [tidyr package](https://cran.r-project.org/package=tidyr) has tools for transforming the data.
* The [ggplot2 package](https://cran.r-project.org/package=ggplot2) for graphics and visuals.
* The [xts package](https://cran.r-project.org/package=xts) is short for 'eXtensible Time Series', which contains tools for working with time series data.

```{r setup, results='hide'}
# Create list of packages needed for this exercise
list.of.packages = c("devtools","readr","tidyr","ggplot2","xts","rmarkdown")
# Check if any have not yet been installed
new.packages = list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
# If any need to be installed, install them
if(length(new.packages)) install.packages(new.packages)
# Urban Institute Mapping package downloaded via GitHub using devtools
library(devtools)
install_github("UrbanInstitute/urbnmapr")
# Load in the packages
library(readr)
library(tidyr)
library(ggplot2)
library(xts)
library(urbnmapr)
```


## Data Download and Cleaning

If you go to the Zillow Research webpage (linked at beginning of Overview), there are a couple dropdown boxes to select different "flavors" of the data. The first option (Data Type) allows you to deviate from the "flagship" ZHVI, which is the all-homes, middle-tier, smoothed, and seasonally-adjusted cut. Other options include filters for different tiers (top or bottom), property types (single-family or condo/co-op), or number of bedrooms. The second dropdown lets you refine the geography level that can range from a national estimate down to the neighborhood level. Although the data downloads provide a monthly time series, it is important to note that finer geography levels (such as neighborhoods) have less data within a given month, and may even have missing values if no residential properties sold in the neighborhood during the particular month. Thus, there are tradeoffs to consider between data frequency and geography level when planning further analysis.


### State-level ZHVI Data

For this analysis, we'll stick to the flagship ZHVI data type. Then to start, we'll begin with the state level data before diving into the finer geography levels. To find the url components in the chunk below, you can use the "Copy link address" tool in your browser after right-clicking on the "Download" button from the webpage. Below, I've partitioned this url into a "base" component (common to all flavors of the data) and an "endpoint" component (`STATESurl` adds the endpoint, which is specific to the particular flavor of the data).

```{r stateurl}
urlbase = "https://files.zillowstatic.com/research/public_csvs/zhvi/"
STATESurl = paste(urlbase,"State_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv", sep="")
STATESurl
```
Now let's use `readr()` to import the csv located at that above url, and then let's display the first few rows with `head()`.

```{r statedownload}
STATESraw = read_csv(STATESurl, show_col_types=FALSE)
head(STATESraw)
```

If we explore the table above, we can start to interpret the structure of the data provided and clean it up. The first observation of note is that we have a "panel" of data, which means that there are two relevant dimensions of variation here: location (spatial) and time (temporal). Another immediate observation about the data is that the StateName variable is entirely empty (RegionName contains the text state names). So our first cleaning step will be to just delete that column from the data frame. Next, we'll transform our data from a "wide" format to a "long" format. The original "wide" format is when the two table dimensions (rows and columns, or observations and variables) are representing the two dimensions of variation in 2-D data. The "long" format transforms the time dimension from being separate columns into a single column, which also elongates the table to where each row is now a state-month observation where each state has many rows corresponding with the time series. This expands our 50-row, 284-column data frame into a 14208-row, 7 column data frame. *Note: this "long" format is particularly useful whenever expanding beyond two dimensions. For example, consider replacing the one spatial dimension, state, with two spatial dimensions, latitude and longitude. Then our data would expand out to three dimensions. Considering that lots of meaningful statistical analysis in economics and finance is multi-dimensional, this is often the best structure to adopt.*

```{r statecleaning}
# Remove empty variable, StateName
STATESraw = STATESraw[,!(names(STATESraw)=="StateName")]
# Expand data frame to long format
STATESlong = pivot_longer(STATESraw,
                          cols=5:ncol(STATESraw),
                          names_to="Date",
                          values_to="ZHVI")
# Reformat date to a date format
STATESlong$Date = as.Date(STATESlong$Date)
# Reformat state name variable, RegionName, into categorical array
STATESlong$RegionName = as.factor(STATESlong$RegionName)
```

Now that we have the state-level data all cleaned up, the next steps on this analysis will require the use of the National ZHVI values. These are included with the Metro-level data. So we'll first download the data for a couple other geography levels to analyze and compare. If you run into any computational limitations, you can stick to just the state/metro level analysis and only run those code chunks. The finer geographies can take some time to clean and analyze if you have an older computer.


### Metro-level ZHVI Data

The default geography on the webpage is for all U.S. Metros, which also comes with a national ZHVI time series. This includes most U.S. metropolitan and micropolitan statistical areas (MSAs, for short). Although it is the [Census](https://www.census.gov/programs-surveys/metro-micro/about.html) who define these terms and indicate that there are 927 total MSAs, the ZHVI data provides fairly good coverage of nearly 900 MSAs, 896 as of June 2023. This corresponds with more than 250,000 MSA-month observations.

```{r metrodownload}
METROSurl = paste(urlbase,"Metro_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv", sep="")
METROSraw = read_csv(METROSurl, show_col_types=FALSE)
nrow(METROSraw)
head(METROSraw)
```

Note that the first observation is the national-level ZHVI, so let's split this off from the MSA-level data into a data frame of its own. Then the remaining rows will compose the MSA-level data frame.

```{r msacleaning}
NATIONAL = pivot_longer(METROSraw[1,],
                          cols=6:ncol(METROSraw),
                          names_to="Date",
                          values_to="ZHVI")
NATIONAL$Date = as.Date(NATIONAL$Date)
METROSlong = pivot_longer(METROSraw[-1,],
                          cols=6:ncol(METROSraw),
                          names_to="Date",
                          values_to="ZHVI")
METROSlong$Date = as.Date(METROSlong$Date)
METROSlong$RegionName = as.factor(METROSlong$RegionName)
```


### County-level ZHVI Data

Next we'll download the county-level data. With over 3000 counties, this produces a cleaned long data frame with more than 850,000 observations.

```{r countydata}
COUNTYurl = paste(urlbase,"County_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv", sep="")
COUNTYraw = read_csv(COUNTYurl, show_col_types=FALSE)
nrow(COUNTYraw)
head(COUNTYraw)
COUNTYlong = pivot_longer(COUNTYraw,
                          cols=10:ncol(COUNTYraw),
                          names_to="Date",
                          values_to="ZHVI")
COUNTYlong$Date = as.Date(COUNTYlong$Date)
# Use RegionID instead of RegionName since latter is not unique identifier
COUNTYlong$RegionID = as.factor(COUNTYlong$RegionID)
```


### ZIP-code-level ZHVI Data

Now to the ZIP-code-level data. With more than 27,000 ZIP codes, this produces a cleaned long data frame with more than 7.5 million observations.

```{r zipdata}
ZIPurl = paste(urlbase,"Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv", sep="")
ZIPraw = read_csv(ZIPurl, show_col_types=FALSE)
nrow(ZIPraw)
head(ZIPraw)
ZIPlong = pivot_longer(ZIPraw,
                          cols=10:ncol(ZIPraw),
                          names_to="Date",
                          values_to="ZHVI")
ZIPlong$Date = as.Date(ZIPlong$Date)
ZIPlong$RegionName = as.factor(ZIPlong$RegionName)
```


### Neighborhood-level ZHVI Data

Lastly, we'll examine the neighborhood-level data. With more than 27,000 ZIP codes, this produces a cleaned long data frame with more than 7.5 million observations.

```{r neighbdata}
NEIGHBurl = paste(urlbase,"Neighborhood_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv", sep="")
NEIGHBraw = read_csv(NEIGHBurl, show_col_types=FALSE)
nrow(NEIGHBraw)
head(NEIGHBraw)
NEIGHBlong = pivot_longer(NEIGHBraw,
                          cols=10:ncol(NEIGHBraw),
                          names_to="Date",
                          values_to="ZHVI")
NEIGHBlong$Date = as.Date(NEIGHBlong$Date)
# Use RegionID instead of RegionName since latter is not unique identifier
NEIGHBlong$RegionID = as.factor(NEIGHBlong$RegionID)
```


## Housing Returns

The next few code chunks will be quite long ones since we will be looping through each of the geographies to transform and model the data. The comments within the loops should hopefully help clarify the logic. First, we pre-allocate the outputs from the loop. This includes the annualized housing returns, fitted regression models for each region's housing returns on the national housing return, as well as the model residuals (*abnormal returns*) and some rolling averages of those variables. Since we need the national housing returns to run the market models, we'll compute those first.


### National Housing Returns

```{r natreturns}
natxts = xts(NATIONAL$ZHVI,order.by=NATIONAL$Date)
natrets = log(as.numeric(natxts)) - log(as.numeric(lag(natxts)))
NATIONAL$AnnGrowth = natrets*12*100
```


### State-level Housing Returns



```{r stateloop}
# Preallocate column for annualized housing returns
STATESlong$AnnGrowth = rep(NA,nrow(STATESlong),1)
# Preallocate list to compile market-model regression results
LinModels_STATE = vector(mode="list", length(levels(STATESlong$RegionName)))
names(LinModels_STATE) = levels(STATESlong$RegionName)
# Preallocate column for regression residuals
STATESlong$RegResids = rep(NA,nrow(STATESlong),1)
# Preallocate columns for rolling averages
STATESlong$ZHVIRollMean = rep(NA,nrow(STATESlong),1)
STATESlong$AnnGrowthRollMean = rep(NA,nrow(STATESlong),1)
STATESlong$RegResidsRollMean = rep(NA,nrow(STATESlong),1)
# Set iteration counter and loop timer
i=1
t=proc.time()
# Loop through each MSA
for (state in levels(STATESlong$RegionName)) {
  # Identify indices for the msa in full data frame
  stateidx = STATESlong$RegionName==state # & !is.na(STATESlong$ZHVI)
  # Extract that subset
  statedf = STATESlong[stateidx,]
  # Impute missing observations (below uses linear interpolation if previously observed)
  statedf$ZHVI = approxfun(1:nrow(statedf),statedf$ZHVI)(1:nrow(statedf))
  # Replace missing with imputed values in main data
  STATESlong$ZHVI[stateidx] = statedf$ZHVI
  # Convert to xts object
  statexts = xts(statedf$ZHVI,order.by=statedf$Date)
  # Compute monthly log returns
  staterets = log(as.numeric(statexts)) - log(as.numeric(lag(statexts)))
  # Convert to annualized percentages and save to full table
  STATESlong$AnnGrowth[stateidx] = staterets*12*100
  # Regress these MSA-level returns on the national returns
  LinModels_STATE[[i]] = lm(STATESlong$AnnGrowth[stateidx]~AnnGrowth,data=NATIONAL)
  # Add model R-squared to saved regression results list
  LinModels_STATE[[i]]$r.squared = summary(LinModels_STATE[[i]])$r.squared
  # Extract model residuals to full table
  STATESlong$RegResids[stateidx & !is.na(STATESlong$ZHVI)] = c(NA,LinModels_STATE[[i]]$residuals)
  # Compute 12-month rolling means for smoother year-over-year values
  STATESlong$ZHVIRollMean[stateidx] = rollmean(statedf$ZHVI,12,fill=NA,align="right")
  STATESlong$AnnGrowthRollMean[stateidx] = rollmean(STATESlong$AnnGrowth[stateidx],12,fill=NA,align="right")
  STATESlong$RegResidsRollMean[stateidx] = rollmean(STATESlong$RegResids[stateidx],12,fill=NA,align="right")
  # Increment counter
  i=i+1
}
proc.time()-t
```

Now that we have the regression residuals, let's output the top 6 and bottom 6 metros with the largest residuals for the most recent month:

```{r staterankings1mo}
STATESnow = STATESlong[STATESlong$Date==max(STATESlong$Date),]
head(STATESnow[order(-STATESnow$RegResids),c(2,3,5,6,7,8)])
head(STATESnow[order(STATESnow$RegResids),c(2,3,5,6,7,8)])
```

Then for a longer-term comparison, let's examine the top 6 and bottom 6 of the residual rolling averages.

```{r staterankings1yr}
head(STATESnow[order(-STATESnow$RegResidsRollMean),c(2,3,5,6,10,11)])
head(STATESnow[order(STATESnow$RegResidsRollMean),c(2,3,5,6,10,11)])
```


### Metro-level Housing Returns

With the MSA-level data, we apply the same procedure to compute the housing returns. As we'd expect, this loop will take a bit longer to run since there are more MSAs than states to loop through.

```{r metroloop}
# Preallocate column for annualized housing returns
METROSlong$AnnGrowth = rep(NA,nrow(METROSlong),1)
# Preallocate list to compile market-model regression results
LinModels_MSA = vector(mode="list", length(levels(METROSlong$RegionName)))
names(LinModels_MSA) = levels(METROSlong$RegionName)
# Preallocate column for regression residuals
METROSlong$RegResids = rep(NA,nrow(METROSlong),1)
# Preallocate columns for rolling averages
METROSlong$ZHVIRollMean = rep(NA,nrow(METROSlong),1)
METROSlong$AnnGrowthRollMean = rep(NA,nrow(METROSlong),1)
METROSlong$RegResidsRollMean = rep(NA,nrow(METROSlong),1)
# Set iteration counter and timer
i=1
t=proc.time()
# Loop through each MSA
for (msa in levels(METROSlong$RegionName)) {
  # Identify indices for the msa in full data frame
  msaidx = METROSlong$RegionName==msa # & !is.na(METROSlong$ZHVI)
  # Extract that subset
  msadf = METROSlong[msaidx,]
  # Impute missing observations (below uses linear interpolation if previously observed)
  msadf$ZHVI = approxfun(1:nrow(msadf),msadf$ZHVI)(1:nrow(msadf))
  # Replace missing with imputed values in main data
  METROSlong$ZHVI[msaidx] = msadf$ZHVI
  # Convert to xts object
  msaxts = xts(msadf$ZHVI,order.by=msadf$Date)
  # Compute monthly log returns
  msarets = log(as.numeric(msaxts)) - log(as.numeric(lag(msaxts)))
  # Convert to annualized percentages and save to full table
  METROSlong$AnnGrowth[msaidx] = msarets*12*100
  # Regress these MSA-level returns on the national returns
  LinModels_MSA[[i]] = lm(METROSlong$AnnGrowth[msaidx]~AnnGrowth,data=NATIONAL)
  # Add model R-squared to saved regression results list
  LinModels_MSA[[i]]$r.squared = summary(LinModels_MSA[[i]])$r.squared
  # Extract model residuals to full table
  METROSlong$RegResids[msaidx & !is.na(METROSlong$ZHVI)] = c(NA,LinModels_MSA[[i]]$residuals)
  # Compute 12-month rolling means for smoother year-over-year values
  METROSlong$ZHVIRollMean[msaidx] = rollmean(msadf$ZHVI,12,fill=NA,align="right")
  METROSlong$AnnGrowthRollMean[msaidx] = rollmean(METROSlong$AnnGrowth[msaidx],12,fill=NA,align="right")
  METROSlong$RegResidsRollMean[msaidx] = rollmean(METROSlong$RegResids[msaidx],12,fill=NA,align="right")
  # Increment counter
  i=i+1
}
proc.time()-t
```

Now that we have the regression residuals, let's output the top 6 and bottom 6 metros with the largest residuals for the most recent month:

```{r msarankings1mo}
METROSnow = METROSlong[METROSlong$Date==max(METROSlong$Date),]
head(METROSnow[order(-METROSnow$RegResids),c(2,3,6,7,8,9)])
head(METROSnow[order(METROSnow$RegResids),c(2,3,6,7,8,9)])
```

Then for a longer-term comparison, let's examine the top 6 and bottom 6 of the residual rolling averages.

```{r msarankings1yr}
head(METROSnow[order(-METROSnow$RegResidsRollMean),c(2,3,6,10,11,12)])
head(METROSnow[order(METROSnow$RegResidsRollMean),c(2,3,6,10,11,12)])
```


### County-level Housing Returns

Now let's do the same for the county-level data: 

```{r countyloop}
# Preallocate column for annualized housing returns
COUNTYlong$AnnGrowth = rep(NA,nrow(COUNTYlong),1)
# Preallocate list to compile market-model regression results
LinModels_COUNTY = vector(mode="list", length(levels(COUNTYlong$RegionID)))
names(LinModels_COUNTY) = levels(COUNTYlong$RegionID)
# Preallocate column for regression residuals
COUNTYlong$RegResids = rep(NA,nrow(COUNTYlong),1)
# Preallocate columns for rolling averages
COUNTYlong$ZHVIRollMean = rep(NA,nrow(COUNTYlong),1)
COUNTYlong$AnnGrowthRollMean = rep(NA,nrow(COUNTYlong),1)
COUNTYlong$RegResidsRollMean = rep(NA,nrow(COUNTYlong),1)
# Set iteration counter and timer
i=1
t=proc.time()
# Loop through each county
#county = uniqueCOUNTYIDs[1]
for (county in levels(COUNTYlong$RegionID)) {
  # Start timer
  #t2 = proc.time()
  # Identify indices for the msa in full data frame
  countyidx = COUNTYlong$RegionID==county # & !is.na(COUNTYlong$ZHVI)
  # Extract that subset
  countydf = COUNTYlong[countyidx,]
  # Impute missing observations (below uses linear interpolation if previously observed)
  # Wrap in if statement to omit any counties with only one observation
  ### East Carroll Parrish, Louisiana had first observation of April 2023
  if (sum(!is.na(countydf$ZHVI)) > 1){
  countydf$ZHVI = approxfun(1:nrow(countydf),countydf$ZHVI)(1:nrow(countydf))
  # Replace missing with imputed values in main data
  COUNTYlong$ZHVI[countyidx] = countydf$ZHVI
  # Convert to xts object
  countyxts = xts(countydf$ZHVI,order.by=countydf$Date)
  # Compute monthly log returns
  countyrets = log(as.numeric(countyxts)) - log(as.numeric(lag(countyxts)))
  # Convert to annualized percentages and save to full table
  COUNTYlong$AnnGrowth[countyidx] = countyrets*12*100
  # Regress these MSA-level returns on the national returns
  LinModels_COUNTY[[i]] = lm(COUNTYlong$AnnGrowth[countyidx]~AnnGrowth,data=NATIONAL)
  # Add model R-squared to saved regression results list
  LinModels_COUNTY[[i]]$r.squared = summary(LinModels_COUNTY[[i]])$r.squared
  # Extract model residuals to full table
  COUNTYlong$RegResids[countyidx & !is.na(COUNTYlong$ZHVI)] = c(NA,LinModels_COUNTY[[i]]$residuals)
  # Compute 12-month rolling means for smoother year-over-year values
  COUNTYlong$ZHVIRollMean[countyidx] = rollmean(countydf$ZHVI,12,fill=NA,align="right")
  COUNTYlong$AnnGrowthRollMean[countyidx] = rollmean(COUNTYlong$AnnGrowth[countyidx],12,fill=NA,align="right")
  COUNTYlong$RegResidsRollMean[countyidx] = rollmean(COUNTYlong$RegResids[countyidx],12,fill=NA,align="right")
  }
  # Stop timer and output result
  #T = proc.time() - t2
  #print(paste("Iteration", as.character(i), "completed in", as.character(round(T[3],2)), "seconds."))
  # Increment counter
  i=i+1
}
proc.time()-t
```

Now that we have the regression residuals, let's output the top 6 and bottom 6 counties with the largest residuals for the most recent month:

```{r countyrankings1mo}
COUNTYnow = COUNTYlong[COUNTYlong$Date==max(COUNTYlong$Date),]
head(COUNTYnow[order(-COUNTYnow$RegResids),c(2,3,6,10,11,12,13)])
head(COUNTYnow[order(COUNTYnow$RegResids),c(2,3,6,10,11,12,13)])
```

Then for a longer-term comparison, let's examine the top 6 and bottom 6 of the residual rolling averages.

```{r countyrankings1yr}
head(COUNTYnow[order(-COUNTYnow$RegResidsRollMean),c(2,3,6,10,14,15,16)])
head(COUNTYnow[order(COUNTYnow$RegResidsRollMean),c(2,3,6,10,14,15,16)])
```


### ZIP-code-level Housing Returns

Now let's do the same for the zip-code-level data:

```{r ziploop}
# Preallocate column for annualized housing returns
ZIPlong$AnnGrowth = rep(NA,nrow(ZIPlong),1)
# Preallocate list to compile market-model regression results
LinModels_ZIP = vector(mode="list", length(levels(ZIPlong$RegionName)))
names(LinModels_ZIP) = levels(ZIPlong$RegionName)
# Preallocate column for regression residuals
ZIPlong$RegResids = rep(NA,nrow(ZIPlong),1)
# Preallocate columns for rolling averages
ZIPlong$ZHVIRollMean = rep(NA,nrow(ZIPlong),1)
ZIPlong$AnnGrowthRollMean = rep(NA,nrow(ZIPlong),1)
ZIPlong$RegResidsRollMean = rep(NA,nrow(ZIPlong),1)
# Set iteration counter and timer
i=1
t=proc.time()
# Loop through each zip
#zip = uniqueZIPIDs[1]
for (zip in levels(ZIPlong$RegionName)) {
  # Identify indices for the msa in full data frame
  zipidx = ZIPlong$RegionName==zip # & !is.na(ZIPlong$ZHVI)
  # Extract that subset
  zipdf = ZIPlong[zipidx,]
  # Impute missing observations (below uses linear interpolation if previously observed)
  if (sum(!is.na(zipdf$ZHVI))>2) {
    zipdf$ZHVI = approxfun(1:nrow(zipdf),zipdf$ZHVI)(1:nrow(zipdf))
  # Replace missing with imputed values in main data
  ZIPlong$ZHVI[zipidx] = zipdf$ZHVI
  # Convert to xts object
  zipxts = xts(zipdf$ZHVI,order.by=zipdf$Date)
  # Compute monthly log returns
  ziprets = log(as.numeric(zipxts)) - log(as.numeric(lag(zipxts)))
  # Convert to annualized percentages and save to full table
  ZIPlong$AnnGrowth[zipidx] = ziprets*12*100
  # Regress these MSA-level returns on the national returns
  LinModels_ZIP[[i]] = lm(ZIPlong$AnnGrowth[zipidx]~AnnGrowth,data=NATIONAL)
  # Add model R-squared to saved regression results list
  LinModels_ZIP[[i]]$r.squared = summary(LinModels_ZIP[[i]])$r.squared
  # Extract model residuals to full table
  ZIPlong$RegResids[zipidx & !is.na(ZIPlong$ZHVI)] = c(NA,LinModels_ZIP[[i]]$residuals)
  # Compute 12-month rolling means for smoother year-over-year values
  ZIPlong$ZHVIRollMean[zipidx] = rollmean(zipdf$ZHVI,12,fill=NA,align="right")
  ZIPlong$AnnGrowthRollMean[zipidx] = rollmean(METROSlong$AnnGrowth[zipidx],12,fill=NA,align="right")
  ZIPlong$RegResidsRollMean[zipidx] = rollmean(METROSlong$RegResids[zipidx],12,fill=NA,align="right")
  }
  # Increment counter
  i=i+1
}
proc.time()-t
```

Now that we have the regression residuals, let's output the top 6 and bottom 6 zip codes with the largest residuals for the most recent month:

```{r ziprankings1mo}
ZIPnow = ZIPlong[ZIPlong$Date==max(ZIPlong$Date),]
head(ZIPnow[order(-ZIPnow$RegResids),c(2,3,6,8,10,11,12,13)])
head(ZIPnow[order(ZIPnow$RegResids),c(2,3,6,8,10,11,12,13)])
```

Then for a longer-term comparison, let's examine the top 6 and bottom 6 of the residual rolling averages.

```{r ziprankings1yr}
head(ZIPnow[order(-ZIPnow$RegResidsRollMean),c(2,3,6,8,10,14,15,16)])
head(ZIPnow[order(ZIPnow$RegResidsRollMean),c(2,3,6,8,10,14,15,16)])
```


### Neighborhood-level Housing Returns

Now let's do the same for the neighborhood-level data:

```{r neighbloop}
# Preallocate column for annualized housing returns
NEIGHBlong$AnnGrowth = rep(NA,nrow(NEIGHBlong),1)
# Preallocate list to compile market-model regression results
LinModels_NEIGHB = vector(mode="list", length(levels(NEIGHBlong$RegionID)))
names(LinModels_NEIGHB) = levels(NEIGHBlong$RegionID)
# Preallocate column for regression residuals
NEIGHBlong$RegResids = rep(NA,nrow(NEIGHBlong),1)
# Preallocate columns for rolling averages
NEIGHBlong$ZHVIRollMean = rep(NA,nrow(NEIGHBlong),1)
NEIGHBlong$AnnGrowthRollMean = rep(NA,nrow(NEIGHBlong),1)
NEIGHBlong$RegResidsRollMean = rep(NA,nrow(NEIGHBlong),1)
# Set iteration counter and timer
i=1
t=proc.time()
# Loop through each neighborhood
#neighb = uniqueNEIGHBIDs[1]
for (neighb in levels(NEIGHBlong$RegionID)) {
  # Identify indices for the msa in full data frame
  neighbidx = NEIGHBlong$RegionID==neighb # & !is.na(NEIGHBlong$ZHVI)
  # Extract that subset
  neighbdf = NEIGHBlong[neighbidx,]
  # Impute missing observations (below uses linear interpolation if previously observed)
  if (sum(!is.na(neighbdf$ZHVI))>2) {
    neighbdf$ZHVI = approxfun(1:nrow(neighbdf),neighbdf$ZHVI)(1:nrow(neighbdf))
  # Replace missing with imputed values in main data
  NEIGHBlong$ZHVI[neighbidx] = neighbdf$ZHVI
  # Convert to xts object
  neighbxts = xts(neighbdf$ZHVI,order.by=neighbdf$Date)
  # Compute monthly log returns
  neighbrets = log(as.numeric(neighbxts)) - log(as.numeric(lag(neighbxts)))
  # Convert to annualized percentages and save to full table
  NEIGHBlong$AnnGrowth[neighbidx] = neighbrets*12*100
  # Regress these MSA-level returns on the national returns
  LinModels_NEIGHB[[i]] = lm(NEIGHBlong$AnnGrowth[neighbidx]~AnnGrowth,data=NATIONAL)
  # Add model R-squared to saved regression results list
  LinModels_NEIGHB[[i]]$r.squared = summary(LinModels_NEIGHB[[i]])$r.squared
  # Extract model residuals to full table
  NEIGHBlong$RegResids[neighbidx & !is.na(NEIGHBlong$ZHVI)] = c(NA,LinModels_NEIGHB[[i]]$residuals)
  # Compute 12-month rolling means for smoother year-over-year values
  NEIGHBlong$ZHVIRollMean[neighbidx] = rollmean(neighbdf$ZHVI,12,fill=NA,align="right")
  NEIGHBlong$AnnGrowthRollMean[neighbidx] = rollmean(NEIGHBlong$AnnGrowth[neighbidx],12,fill=NA,align="right")
  NEIGHBlong$RegResidsRollMean[neighbidx] = rollmean(NEIGHBlong$RegResids[neighbidx],12,fill=NA,align="right")
  }
  # Increment counter
  i=i+1
}
proc.time()-t
```

Now that we have the regression residuals, let's output the top 6 and bottom 6 counties with the largest residuals for the most recent month:

```{r neighbrankings1mo}
NEIGHBnow = NEIGHBlong[NEIGHBlong$Date==max(NEIGHBlong$Date),]
head(NEIGHBnow[order(-NEIGHBnow$RegResids),c(2,3,6,8,10,11,12,13)])
head(NEIGHBnow[order(NEIGHBnow$RegResids),c(2,3,6,8,10,11,12,13)])
```

Then for a longer-term comparison, let's examine the top 6 and bottom 6 of the residual rolling averages.

```{r neighbrankings1yr}
head(NEIGHBnow[order(-NEIGHBnow$RegResidsRollMean),c(2,3,6,8,10,14,15,16)])
head(NEIGHBnow[order(NEIGHBnow$RegResidsRollMean),c(2,3,6,8,10,14,15,16)])
```


## Mapping Geographical Data

Now that we've computed several measures of housing returns across various geography levels, let's explore some tools to generate heat maps to better visualize the data.

```{r mapmerge}
#states = urbnmapr::states
#counties = urbnmapr::counties
now = max(COUNTYlong$Date)
COUNTYnow = COUNTYlong[COUNTYlong$Date==now,]
COUNTYnow$county_fips = paste(as.character(COUNTYnow$StateCodeFIPS), as.character(COUNTYnow$MunicipalCodeFIPS), sep="")
library(dplyr) # Wait til here to load dplyr because it will override the lag function from xts package
COUNTYmerge = left_join(counties, COUNTYnow, by="county_fips")
```

```{r mapplots1}
COUNTYmerge |>
  ggplot(aes(long, lat, group = group, fill = AnnGrowth)) +
  geom_polygon(color = NA) +
  coord_map(projection = "albers", lat0 = 39, lat1 = 45) +
  labs(fill = "Annualized Housing Market Growth Over Past 1 Month") +
  scale_fill_gradient2(
    low = "blue", mid = "white", high = "red",
    midpoint = 0, na.value = "gray"
  )

COUNTYmerge |>
  ggplot(aes(long, lat, group = group, fill = RegResids)) +
  geom_polygon(color = NA) +
  coord_map(projection = "albers", lat0 = 39, lat1 = 45) +
  labs(fill = "Annualized Housing Market Alpha Over Past 1 Month") +
  scale_fill_gradient2(
    low = "blue", mid = "white", high = "red",
    midpoint = 0, na.value = "gray"
  )

COUNTYmerge |>
  ggplot(aes(long, lat, group = group, fill = AnnGrowthRollMean)) +
  geom_polygon(color = NA) +
  coord_map(projection = "albers", lat0 = 39, lat1 = 45) +
  labs(fill = "Annualized Housing Market Growth Over Past 12 Months") +
  scale_fill_gradient2(
    low = "blue", mid = "white", high = "red",
    midpoint = 0, na.value = "gray"
  )

COUNTYmerge |>
  ggplot(aes(long, lat, group = group, fill = RegResidsRollMean)) +
  geom_polygon(color = NA) +
  coord_map(projection = "albers", lat0 = 39, lat1 = 45) +
  labs(fill = "Annualized Housing Market Alpha Over Past 12 Months") +
  scale_fill_gradient2(
    low = "blue", mid = "white", high = "red",
    midpoint = 0, na.value = "gray"
  )
```

```{r mapplots2}
COUNTYmerge |>
  ggplot(aes(long, lat, group = group, fill = RegResidsRollMean)) +
  geom_polygon(color = NA) +
  #geom_polygon(data = states, mapping = aes(long, lat, group = group),
  #             fill = NA, color = "#ffffff") +
  coord_map(projection = "albers", lat0 = 39, lat1 = 45) +
  theme(legend.title = element_text(),
        legend.key.width = unit(.5, "in")) +
  labs(fill = "Annualized Housing Market Alpha Over Past 12 Months") +
  scale_fill_gradient2(
    low = "blue", mid = "white", high = "red",
    midpoint = 0, na.value = "gray"
    )

COUNTYmerge |>
  filter(state_name =="Missouri") |>
  ggplot(mapping = aes(long, lat, group = group, fill = RegResidsRollMean)) +
  geom_polygon(color = NA) +
  #geom_polygon(color = "#ffffff", linewidth = .25) +
  coord_map(projection = "albers", lat0 = 39, lat1 = 45) +
  theme(legend.title = element_text(),
        legend.key.width = unit(.5, "in")) +
  labs(fill = "Annualized Housing Market Alpha Over Past 12 Months (Missouri)") +
  scale_fill_gradient2(
    low = "blue", mid = "white", high = "red",
    midpoint = 0, na.value = "gray"
  )
```


## Missouri Housing Analysis

Now that we've gotten all the data cleaned and have run some preliminary regressions to split 

```{r mosubsets}
MOMETROS = METROSlong[METROSlong$StateName=="MO",]
MOCOUNTY = COUNTYlong[COUNTYlong$StateName=="MO",]
MOZIPS = ZIPlong[ZIPlong$StateName=="MO",]
MONEIGHB = NEIGHBlong[NEIGHBlong$StateName=="MO",]
```


```{r plottest}
ggplot(MOMETROS,aes(x=Date, color=RegionName))+
  geom_col(aes(y=AnnGrowth))+
  geom_line(aes(y=AnnGrowthRollMean)) +
  ggtitle("Continuously Compounded Growth in ZHVI Across Missouri Metros") +
  xlab("") +
  ylab("Annualized Growth Rate")


ggplot(MOMETROS,aes(x=Date, color=RegionName))+
  geom_col(aes(y=RegResids))+
  geom_line(aes(y=RegResidsRollMean)) +
  ggtitle("Market Model Regression Residuals for ZHVI Growth Across Missouri Metros") +
  xlab("") +
  ylab("Annualized Growth Rate")
```

```{r plottest2}
ggplot(MOMETROS,aes(x=Date,y=ZHVI,color=RegionName))+
  geom_line()

ggplot(MOMETROS,aes(x=Date,y=ZHVIRollMean,color=RegionName))+
  geom_line()
```


## Save Final Workspace

```{r savedata}
save.image("finalworkspace.RData")
```

