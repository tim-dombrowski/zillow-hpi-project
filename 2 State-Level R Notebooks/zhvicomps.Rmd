---
title: "State-Level ZHVI Comparisons"
output: html_notebook
---

## Preliminary Work: Install/Load Packages

To try and ensure that this R Notebook will run successfully, we'll use the [renv package](https://cran.r-project.org/web/packages/renv/index.html) to create a project-specific library of packages. This will allow us to install the packages that we need for this project without affecting any other projects that we may be working on. Additionally, the project library will track the specific versions of the dependency packages so that any updates to those packages will not break this project.

The code chunk below will first install the renv package if it is not already installed. Then we will load the package. Next, we'll use the `restore()` function to install any packages listed in the renv.lock file. Once these packages are installed, we can load them into the R session using the `library()` commands. Below the code chunk, we'll list out the packages that will be used in the project demo. And if you run into any trouble using renv, then you can use the second code chunk below and that should be an even more reliable approach to install the required packages.

```{r setup, results='hide', message=FALSE}
# Install renv package if not already installed
if(!"renv" %in% installed.packages()[,"Package"]) install.packages("renv")
# Load renv package
library(renv)
# Use restore() to install any packages listed in the renv.lock file
renv::restore(clean=TRUE, lockfile="../renv.lock")
# Load in the packages
library(tidyverse)
library(fst)
library(xts)
#library(urbnmapr)
```

* The [readr package](https://cran.r-project.org/package=readr) is a common package for reading in data files. After installing, the RStudio Environment tab will have a new option to import data sets that uses this package. It will even generate the R code for you to copy and paste into your script.
* The [tidyr package](https://cran.r-project.org/package=tidyr) has tools for transforming the data.
* The [ggplot2 package](https://cran.r-project.org/package=ggplot2) for graphics and visuals.
* The [xts package](https://cran.r-project.org/package=xts) is short for 'eXtensible Time Series', which contains tools for working with time series data.
* The [urbnmapr package](https://github.com/UrbanInstitute/urbnmapr) has mapping data for various geographies.
  * This package is not available through the standard CRAN repository, rather through GitHub. This information is contained within the renv.lock file; however, if you wish to download manually below, the [devtools package](https://cran.r-project.org/package=devtools) contains the `install_github()` function, which can be used to install the package from GitHub.
* The [rmarkdown package](https://cran.r-project.org/package=rmarkdown) is used to generate this R Notebook.

Since the rmarkdown functionality is built into RStudio, this last one is automatically loaded when you open RStudio. So no need to use the `library()` function for it. Another observation to make about the code chunk above is that it is labeled as `setup`, which is a special name, which the R Notebook will recognize and automatically run prior to running any other code chunk. This is useful for loading in packages and setting up other global options that will be used throughout the notebook. 

Then if you wish to try and update the versions of the various R packages in the lock file, you can use the `renv::update()` function to update the packages in the project library. However, it is possible that these updates could break the code in this notebook. If so, you may need to adapt the code to work with the updated packages.

My recommendation is to first run through the code using the versions of the packages in the lock file. Then if you want to try and update the packages, you can do so and then run through the code again to see if it still works. If not, you can always revert back to the lock file versions using the `renv::restore()` function.

If you update the packages and get everything working successfully, then you can update the lock file using the `renv::snapshot()` function. This will update the lock file with the versions of the packages that are currently installed in the project library. Then you can commit the updated lock file to the repository so that others can use the updated versions of the packages.

### Alternative Package Installation Code

If you run into any trouble using renv in the code chunk above, then you can use the code chunk below to install the required packages for this analysis. This method will first check if you have already installed the packages. If any are missing, it will then install them. Then it will load the packages into the R session. A potential flaw in this approach compared to using renv is that it will simply install the latest versions of the packages, which could potentially break some of the code in this notebook if any of the updates aren't backwards compatible. 

As long as you have downloaded the entire project repository, the renv chunk above will likely be managing the packages. Thus, the `eval=FALSE` option is used to prevent this chunk from running unless manually executed. So if you only downloaded this one Rmd file, this code chunk should take care of installing the packages for you.

```{r setup2, results='hide', message=FALSE, eval=FALSE}
# Create list of packages needed for this exercise
list.of.packages = c("devtools","tidyverse","fst","xts","rmarkdown")
# Check if any have not yet been installed
new.packages = list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
# If any need to be installed, install them
if(length(new.packages)) install.packages(new.packages)
# Urban Institute Mapping package downloaded via GitHub using devtools
#library(devtools)
#install_github("UrbanInstitute/urbnmapr")
# Load in the packages
library(tidyverse)
library(fst)
library(xts)
#library(urbnmapr)
```


## Import State-Level ZHVI Data








## Data Background

If you go to the [Zillow Research webpage](https://www.zillow.com/research/data/), there are several types datasets available for exploration. Within each of these datasets, there are dropdown boxes to select different variations ('Data Type') and 'Geography' levels. For this exercise, we will stick with the flagship Zillow Home Value Index (ZHVI) data, which is the all-homes, middle-tier, smoothed, and seasonally-adjusted variant of the ZHVI. Then we will explore this data across several geography levels, including state, metro, county, ZIP code, and neighborhood.

Although the data downloads provide a monthly time series, it is important to note that finer geography levels (such as neighborhoods) have less data within a given month, and may even have missing values if no residential properties sold in the neighborhood during the particular month. Thus, there are tradeoffs to consider between data frequency and geography level when planning further analysis.

The final note before we begin the analysis is that the finer geography levels (ZIP code and neighborhood) can take a long time to clean and analyze if you have an older computer. If you run into any computational limitations, you can stick to just the state/metro level analysis and only run those code chunks. I've included code to time each of the loops so that you can get a sense of the runtime for each of the geographies. There is some potential to parallelize the loops to speed up the process.

We'll start with the state-level data before moving on to the metro-level data, which also contains the national ZHVI estimates. Then we'll start doing some analysis of those smaller datasets before moving on to the county, ZIP code, and neighborhood levels. 


## Data Downloads

### State-level ZHVI Data

The code chunk below contains the url for the state-level ZHVI data. If you wanted to re-write this chunk, you would need to go to the [Zillow Research webpage](https://www.zillow.com/research/data/), select the "State" geography level under the first 'Home Values' category. Then leave the 'Data Type' as the default option and right-click on the "Download" button. If you select 'Copy link address', you can then paste that to view the url of the data download (it should end with .csv). The chunk below splits this full url into a 'base', which is common to all of the Zillow datasets, and an 'endpoint', which specifies the particular table of data to retrieve.

```{r stateurl}
urlbase = "https://files.zillowstatic.com/research/public_csvs/zhvi/"
STATESurl = paste(urlbase,"State_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv", sep="")
STATESurl
```

Now let's use `read_csv()` from the readr package to import the csv located at the url, and then let's display the first few rows with `head()`.

```{r statedownload}
STATESraw = read_csv(STATESurl, show_col_types=FALSE)
head(STATESraw)
```

If we explore the table above, we can start to interpret the structure of the data provided and clean it up. The first observation of note is that we have a [panel of data](https://search.brave.com/search?q=panel+data), which means that there are two relevant dimensions of variation here: location (spatial) and time (temporal). 

Another immediate observation about the data is that the StateName variable is entirely empty (RegionName contains the text state names). So our first cleaning step will be to just delete that column from the data frame. 

Next, we'll transform our data from a 'wide' format to a 'long' format. The 'wide' format is when the two table dimensions (rows and columns, or observations and variables) represent the two dimensions of variation in the panel. The 'long' format transforms the time dimension from being separate columns into a single column. This also elongates the table to where each row is now a state-month observation where each state has many rows corresponding with the time series. This expands our 50-row, 284-column data frame into a 14208-row, 7 column data frame. 

*Note: this 'long' format is particularly useful whenever expanding beyond two dimensions. For example, consider replacing the one spatial dimension, state, with two spatial dimensions, latitude and longitude. Then our data would expand out to three dimensions. Considering that lots of meaningful statistical analysis in economics and finance is multi-dimensional, this is often the best structure to adopt.*

```{r statecleaning}
# Remove empty variable, StateName
STATESraw = STATESraw[,!(names(STATESraw)=="StateName")]
# Expand data frame to long format
STATESlong = pivot_longer(STATESraw,
                          cols=5:ncol(STATESraw),
                          names_to="Date",
                          values_to="ZHVI")
# Reformat date to a date format
STATESlong$Date = as.Date(STATESlong$Date)
# Reformat state name variable, RegionName, into categorical array
STATESlong$RegionName = as.factor(STATESlong$RegionName)
```

Now that we have the state-level data all cleaned up, the next steps on this analysis will require the use of the National ZHVI values. These are included with the Metro-level data. Then we'll move to some analysis of those datasets before diving into the finer geography levels, which are larger datasets and more time consuming to analyze.

### Metro-level ZHVI Data

The default geography on the webpage is for all U.S. Metros, which also comes with a national ZHVI time series. This includes most U.S. metropolitan and micropolitan statistical areas (MSAs, for short). The MSA classification is defined by the [Census](https://www.census.gov/programs-surveys/metro-micro/about.html) and indicate that there are 925 total MSAs, as of July 2023. The ZHVI data provides fairly good coverage spanning more than 95% of MSAs, 895 as of February 2024. This corresponds with more than 250,000 MSA-month observations.

```{r metrodownload}
METROSurl = paste(urlbase,"Metro_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv", sep="")
METROSraw = read_csv(METROSurl, show_col_types=FALSE)
nrow(METROSraw)
head(METROSraw)
```

Note that the first observation is the national-level ZHVI, so let's split this off from the MSA-level data into a data frame of its own. Then the remaining rows will compose the MSA-level data frame.

```{r msacleaning}
NATIONAL = pivot_longer(METROSraw[1,],
                          cols=6:ncol(METROSraw),
                          names_to="Date",
                          values_to="ZHVI")
NATIONAL$Date = as.Date(NATIONAL$Date)
METROSlong = pivot_longer(METROSraw[-1,],
                          cols=6:ncol(METROSraw),
                          names_to="Date",
                          values_to="ZHVI")
METROSlong$Date = as.Date(METROSlong$Date)
METROSlong$RegionName = as.factor(METROSlong$RegionName)
```


## Housing Returns

The next step in the analysis is to transform the ZHVI data into measures of housing returns. The rationale for this step is rooted in the statistical concept of [time series stationarity](https://search.brave.com/search?q=time+series+stationarity). Check out the [Bitcoin Time Series Analysis Project](https://github.com/tim-dombrowski/bitcoin-projects/tree/main/Time%20Series%20Analysis%20R%20Notebook) for a more detailed exploration of this concept.

### National Housing Returns

Since the national ZHVI data is just a single time series, we can use this as a concise example of the process to transform the ZHVI data into housing returns. The first step is to convert the ZHVI data into an xts object. Then we can compute the monthly log returns, which is difference between the natural logarithms (`log()`) of the current month's ZHVI to the previous month's ZHVI (`lag()`). This is then multiplied by 12 to annualize the returns, and then by 100 to switch the units to percentages instead of decimals. This is then saved to the `NATIONAL` data frame as a new column, `AnnGrowth`.

```{r natreturns}
natxts = xts(NATIONAL$ZHVI,order.by=NATIONAL$Date)
natrets = log(as.numeric(natxts)) - log(as.numeric(lag(natxts)))
NATIONAL$AnnGrowth = natrets*12*100
```

### State-level Housing Returns

The next couple code chunks will be a bit long ones since we will be looping through each of the states to transform to returns and model the data. The comments within the loops should hopefully help clarify the logic. 

First, we pre-allocate the outputs from the loop. This includes the annualized housing returns, fitted regression models for each region's housing returns on the national housing return, as well as the model residuals (*abnormal returns*) and some rolling averages of those variables. Then we'll loop through each state to compute and extract all those results.

Since there are only 50 states, this loop should run fairly quickly (within 1 second or just a couple seconds).

```{r stateloop}
# Preallocate column for annualized housing returns
STATESlong$AnnGrowth = rep(NA,nrow(STATESlong),1)
# Preallocate list to compile market-model regression results
LinModels_STATE = vector(mode="list", length(levels(STATESlong$RegionName)))
names(LinModels_STATE) = levels(STATESlong$RegionName)
# Preallocate column for regression residuals
STATESlong$RegResids = rep(NA,nrow(STATESlong),1)
# Preallocate columns for rolling averages
STATESlong$ZHVIRollMean = rep(NA,nrow(STATESlong),1)
STATESlong$AnnGrowthRollMean = rep(NA,nrow(STATESlong),1)
STATESlong$RegResidsRollMean = rep(NA,nrow(STATESlong),1)
# Set iteration counter and loop timer
i=1
t=proc.time()
# Loop through each MSA
for (state in levels(STATESlong$RegionName)) {
  # Identify indices for the msa in full data frame
  stateidx = STATESlong$RegionName==state # & !is.na(STATESlong$ZHVI)
  # Extract that subset
  statedf = STATESlong[stateidx,]
  # Impute missing observations (below uses linear interpolation if previously observed)
  statedf$ZHVI = approxfun(1:nrow(statedf),statedf$ZHVI)(1:nrow(statedf))
  # Replace missing with imputed values in main data
  STATESlong$ZHVI[stateidx] = statedf$ZHVI
  # Convert to xts object
  statexts = xts(statedf$ZHVI,order.by=statedf$Date)
  # Compute monthly log returns
  staterets = log(as.numeric(statexts)) - log(as.numeric(lag(statexts)))
  # Convert to annualized percentages and save to full table
  STATESlong$AnnGrowth[stateidx] = staterets*12*100
  # Regress these MSA-level returns on the national returns
  LinModels_STATE[[i]] = lm(STATESlong$AnnGrowth[stateidx]~AnnGrowth,data=NATIONAL)
  # Add model R-squared to saved regression results list
  LinModels_STATE[[i]]$r.squared = summary(LinModels_STATE[[i]])$r.squared
  # Extract model residuals to full table
  STATESlong$RegResids[stateidx & !is.na(STATESlong$ZHVI)] = c(NA,LinModels_STATE[[i]]$residuals)
  # Compute 12-month rolling means for smoother year-over-year values
  STATESlong$ZHVIRollMean[stateidx] = rollmean(statedf$ZHVI,12,fill=NA,align="right")
  STATESlong$AnnGrowthRollMean[stateidx] = rollmean(STATESlong$AnnGrowth[stateidx],12,fill=NA,align="right")
  STATESlong$RegResidsRollMean[stateidx] = rollmean(STATESlong$RegResids[stateidx],12,fill=NA,align="right")
  # Increment counter
  i=i+1
}
proc.time()-t
```

Now let's do a quick examination of the results from this loop. Since we have the regression residuals from modeling each state on the national returns, let's output the top 6 and bottom 6 metros for the most recent month:

```{r staterankings1mo}
STATESnow = STATESlong[STATESlong$Date==max(STATESlong$Date),]
head(STATESnow[order(-STATESnow$RegResids),c(2,3,5,6,7,8)])
head(STATESnow[order(STATESnow$RegResids),c(2,3,5,6,7,8)])
```

Then for a longer-term comparison, let's examine the top 6 and bottom 6 of the residual rolling averages. This is effectively a comparison of housing returns over the past 12 months.

```{r staterankings1yr}
head(STATESnow[order(-STATESnow$RegResidsRollMean),c(2,3,5,6,10,11)])
head(STATESnow[order(STATESnow$RegResidsRollMean),c(2,3,5,6,10,11)])
```

### Metro-level Housing Returns

With the MSA-level data, we apply the same procedure to compute the housing returns. As we'd expect, this loop will take a bit longer to run since there are more MSAs than states to loop through. Though, this one should still run fairly quickly (within a minute or two).

```{r metroloop}
# Preallocate column for annualized housing returns
METROSlong$AnnGrowth = rep(NA,nrow(METROSlong),1)
# Preallocate list to compile market-model regression results
LinModels_MSA = vector(mode="list", length(levels(METROSlong$RegionName)))
names(LinModels_MSA) = levels(METROSlong$RegionName)
# Preallocate column for regression residuals
METROSlong$RegResids = rep(NA,nrow(METROSlong),1)
# Preallocate columns for rolling averages
METROSlong$ZHVIRollMean = rep(NA,nrow(METROSlong),1)
METROSlong$AnnGrowthRollMean = rep(NA,nrow(METROSlong),1)
METROSlong$RegResidsRollMean = rep(NA,nrow(METROSlong),1)
# Set iteration counter and timer
i=1
t=proc.time()
# Loop through each MSA
for (msa in levels(METROSlong$RegionName)) {
  # Identify indices for the msa in full data frame
  msaidx = METROSlong$RegionName==msa # & !is.na(METROSlong$ZHVI)
  # Extract that subset
  msadf = METROSlong[msaidx,]
  # Impute missing observations (below uses linear interpolation if previously observed)
  msadf$ZHVI = approxfun(1:nrow(msadf),msadf$ZHVI)(1:nrow(msadf))
  # Replace missing with imputed values in main data
  METROSlong$ZHVI[msaidx] = msadf$ZHVI
  # Convert to xts object
  msaxts = xts(msadf$ZHVI,order.by=msadf$Date)
  # Compute monthly log returns
  msarets = log(as.numeric(msaxts)) - log(as.numeric(lag(msaxts)))
  # Convert to annualized percentages and save to full table
  METROSlong$AnnGrowth[msaidx] = msarets*12*100
  # Regress these MSA-level returns on the national returns
  LinModels_MSA[[i]] = lm(METROSlong$AnnGrowth[msaidx]~AnnGrowth,data=NATIONAL)
  # Add model R-squared to saved regression results list
  LinModels_MSA[[i]]$r.squared = summary(LinModels_MSA[[i]])$r.squared
  # Extract model residuals to full table
  METROSlong$RegResids[msaidx & !is.na(METROSlong$ZHVI)] = c(NA,LinModels_MSA[[i]]$residuals)
  # Compute 12-month rolling means for smoother year-over-year values
  METROSlong$ZHVIRollMean[msaidx] = rollmean(msadf$ZHVI,12,fill=NA,align="right")
  METROSlong$AnnGrowthRollMean[msaidx] = rollmean(METROSlong$AnnGrowth[msaidx],12,fill=NA,align="right")
  METROSlong$RegResidsRollMean[msaidx] = rollmean(METROSlong$RegResids[msaidx],12,fill=NA,align="right")
  # Increment counter
  i=i+1
}
proc.time()-t
```

Just as we did for the state-level data, let's output the top 6 and bottom 6 metros from the market models for the most recent month:

```{r msarankings1mo}
METROSnow = METROSlong[METROSlong$Date==max(METROSlong$Date),]
head(METROSnow[order(-METROSnow$RegResids),c(2,3,6,7,8,9)])
head(METROSnow[order(METROSnow$RegResids),c(2,3,6,7,8,9)])
```

Then for a longer-term comparison, let's examine the top 6 and bottom 6 of the residual rolling averages.

```{r msarankings1yr}
head(METROSnow[order(-METROSnow$RegResidsRollMean),c(2,3,6,10,11,12)])
head(METROSnow[order(METROSnow$RegResidsRollMean),c(2,3,6,10,11,12)])
```


## Mapping Examples

The `urbnmapr` package contains a variety of mapping data for different geographies (primarily at the state and county level). We'll use this package to map the state-level ZHVI data.

### Mapping State-level Data

First, we'll import the mapping data and merge it with the state-level ZHVI data. Then we'll plot the map with the ZHVI values as the fill color. Some other features for the plot are generating the black state outlines with `geom_polygon()`, performing an [Albers projection](https://search.brave.com/search?q=Albers+projection) with `coord_map()` to create a map with equal areas across latitudes, and labeling the fill color and map title with `labs()`. Then the `scale_fill_gradient2()` function is used to create a color gradient for the fill color. The `midpoint` argument is used to specify the color that corresponds with the mean ZHVI value. The `na.value` argument is used to specify the color for missing values. This is also where we can specify that the legend should display the values in dollars.

```{r statemap}
# Download map data
#STATESmap = urbnmapr::states
# Merge ZHVI data with map data
STATESmerge = merge(STATESmap, STATESnow, by.x="state_name", by.y="RegionName")
# Plot the map
STATESmerge |>
  ggplot(aes(x=long, 
             y=lat, 
             group=group, 
             fill=ZHVI)) +
  # Generate black outline to states
  geom_polygon(color="#000000", linewidth=.25) +
  # Perform Albers projection
  coord_map(projection="albers", 
            lat0=39,
            lat1=45) +
  # Label the fill color
  labs(fill="ZHVI", 
       title=paste("ZHVI by State as of ",as.character(STATESnow$Date[1]),sep="")) +
  # Create a color gradient for the fill color and specify the midpoint and missing value color
  # Also, specify that the legend should display dollars
  scale_fill_gradient2(low="blue", 
                       mid="white", 
                       high="red",
                       midpoint=mean(STATESnow$ZHVI,na.rm=TRUE),
                       na.value="gray",
                       labels=scales::dollar_format())
```

```{r statemaps}
# Plot the map
STATESmerge |>
  ggplot(aes(x=long, 
             y=lat, 
             group=group, 
             fill=AnnGrowthRollMean/100)) +
  # Generate black outline to states
  geom_polygon(color="#000000", linewidth=.25) +
  # Perform Albers projection
  coord_map(projection="albers", 
            lat0=39,
            lat1=45) +
  # Label the fill color
  labs(fill="AnnGrowth", 
       title=paste("Average Annualized ZHVI Growth by State for 12 months ended ",as.character(STATESnow$Date[1]),sep="")) +
  # Create a color gradient for the fill color and specify the midpoint and missing value color
  # Also, specify that the legend should display dollars
  scale_fill_gradient2(low="blue", 
                       mid="white", 
                       high="red",
                       midpoint=0,
                       na.value="gray",
                       labels=scales::percent_format())

# Plot the map
STATESmerge |>
  ggplot(aes(x=long, 
             y=lat, 
             group=group, 
             fill=RegResidsRollMean/100)) +
  # Generate black outline to states
  geom_polygon(color="#000000", linewidth=.25) +
  # Perform Albers projection
  coord_map(projection="albers", 
            lat0=39,
            lat1=45) +
  # Label the fill color
  labs(fill="RegResids", 
       title=paste("ZHVI Market Model Residuals by State for 12 months ended ",as.character(STATESnow$Date[1]),sep="")) +
  # Create a color gradient for the fill color and specify the midpoint and missing value color
  # Also, specify that the legend should display dollars
  scale_fill_gradient2(low="blue", 
                       mid="white", 
                       high="red",
                       midpoint=0,
                       na.value="gray",
                       labels=scales::percent_format())
```

**The analysis below this point is still a work-in-progress!**

## County-Level ZHVI Data

Next we'll download the county-level data. With over 3000 counties, this produces a cleaned long data frame with more than 850,000 observations.

```{r countydata}
COUNTYurl = paste(urlbase,"County_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv", sep="")
COUNTYraw = read_csv(COUNTYurl, show_col_types=FALSE)
nrow(COUNTYraw)
head(COUNTYraw)
COUNTYlong = pivot_longer(COUNTYraw,
                          cols=10:ncol(COUNTYraw),
                          names_to="Date",
                          values_to="ZHVI")
COUNTYlong$Date = as.Date(COUNTYlong$Date)
# Use RegionID instead of RegionName since latter is not unique identifier
COUNTYlong$RegionID = as.factor(COUNTYlong$RegionID)
```

### County-Level Housing Returns

Now let's do the same cleaning and analysis for the county-level data. This loop will take a bit longer to run since there are over 3,000 counties to loop through. This one may take a few minutes to run.

```{r countyloop}
# Preallocate column for annualized housing returns
COUNTYlong$AnnGrowth = rep(NA,nrow(COUNTYlong),1)
# Preallocate list to compile market-model regression results
LinModels_COUNTY = vector(mode="list", length(levels(COUNTYlong$RegionID)))
names(LinModels_COUNTY) = levels(COUNTYlong$RegionID)
# Preallocate column for regression residuals
COUNTYlong$RegResids = rep(NA,nrow(COUNTYlong),1)
# Preallocate columns for rolling averages
COUNTYlong$ZHVIRollMean = rep(NA,nrow(COUNTYlong),1)
COUNTYlong$AnnGrowthRollMean = rep(NA,nrow(COUNTYlong),1)
COUNTYlong$RegResidsRollMean = rep(NA,nrow(COUNTYlong),1)
# Set iteration counter and timer
i=1
t=proc.time()
# Loop through each county
#county = uniqueCOUNTYIDs[1]
for (county in levels(COUNTYlong$RegionID)) {
  # Start timer
  #t2 = proc.time()
  # Identify indices for the msa in full data frame
  countyidx = COUNTYlong$RegionID==county # & !is.na(COUNTYlong$ZHVI)
  # Extract that subset
  countydf = COUNTYlong[countyidx,]
  # Impute missing observations (below uses linear interpolation if previously observed)
  # Wrap in if statement to omit any counties with only one observation
  ### East Carroll Parrish, Louisiana had first observation of April 2023
  if (sum(!is.na(countydf$ZHVI)) > 1){
  countydf$ZHVI = approxfun(1:nrow(countydf),countydf$ZHVI)(1:nrow(countydf))
  # Replace missing with imputed values in main data
  COUNTYlong$ZHVI[countyidx] = countydf$ZHVI
  # Convert to xts object
  countyxts = xts(countydf$ZHVI,order.by=countydf$Date)
  # Compute monthly log returns
  countyrets = log(as.numeric(countyxts)) - log(as.numeric(lag(countyxts)))
  # Convert to annualized percentages and save to full table
  COUNTYlong$AnnGrowth[countyidx] = countyrets*12*100
  # Regress these MSA-level returns on the national returns
  LinModels_COUNTY[[i]] = lm(COUNTYlong$AnnGrowth[countyidx]~AnnGrowth,data=NATIONAL)
  # Add model R-squared to saved regression results list
  LinModels_COUNTY[[i]]$r.squared = summary(LinModels_COUNTY[[i]])$r.squared
  # Extract model residuals to full table
  COUNTYlong$RegResids[countyidx & !is.na(COUNTYlong$ZHVI)] = c(NA,LinModels_COUNTY[[i]]$residuals)
  # Compute 12-month rolling means for smoother year-over-year values
  COUNTYlong$ZHVIRollMean[countyidx] = rollmean(countydf$ZHVI,12,fill=NA,align="right")
  COUNTYlong$AnnGrowthRollMean[countyidx] = rollmean(COUNTYlong$AnnGrowth[countyidx],12,fill=NA,align="right")
  COUNTYlong$RegResidsRollMean[countyidx] = rollmean(COUNTYlong$RegResids[countyidx],12,fill=NA,align="right")
  }
  # Stop timer and output result
  #T = proc.time() - t2
  #print(paste("Iteration", as.character(i), "completed in", as.character(round(T[3],2)), "seconds."))
  # Increment counter
  i=i+1
}
proc.time()-t
```

Now let's examine the regression residuals. Let's output the top 6 and bottom 6 counties with the largest residuals for the most recent month:

```{r countyrankings1mo}
COUNTYnow = COUNTYlong[COUNTYlong$Date==max(COUNTYlong$Date),]
head(COUNTYnow[order(-COUNTYnow$RegResids),c(2,3,6,10,11,12,13)])
head(COUNTYnow[order(COUNTYnow$RegResids),c(2,3,6,10,11,12,13)])
```

Then for a longer-term comparison, let's examine the top 6 and bottom 6 of the residual rolling averages.

```{r countyrankings1yr}
head(COUNTYnow[order(-COUNTYnow$RegResidsRollMean),c(2,3,6,10,14,15,16)])
head(COUNTYnow[order(COUNTYnow$RegResidsRollMean),c(2,3,6,10,14,15,16)])
```

### Mapping U.S. Counties

Now let's map the county-level ZHVI data. To avoid any potential mismatches of matching by county name, we'll use the County FIPS code to uniquely identify each county for the merge. The Zillow data has this split into two pieces: the first two digits are the state code and the last three digits are the county code. We'll create a full 5-digit FIPS code for each county and then merge the ZHVI data with the map data. Then we'll plot the map with the ZHVI values as the fill color.

```{r countymap}
# Download map data
#COUNTYmap = urbnmapr::counties
# Create full 5-digit FIPS code for each county
COUNTYnow$county_fips = paste(as.character(COUNTYnow$StateCodeFIPS), as.character(COUNTYnow$MunicipalCodeFIPS), sep="")
# Merge ZHVI data with map data
COUNTYmerge = merge(COUNTYmap, COUNTYnow, by="county_fips")
# Plot the map
COUNTYmerge |>
  ggplot(aes(x=long, 
             y=lat, 
             group=group, 
             fill=ZHVI)) +
  # Generate black outline to states
  geom_polygon(color=NA) +
  # Perform Albers projection
  coord_map(projection="albers", 
            lat0=39,
            lat1=45) +
  # Label the fill color
  labs(fill="ZHVI", 
       title=paste("ZHVI by County as of ",as.character(STATESnow$Date[1]),sep="")) +
  # Create a color gradient for the fill color and specify the midpoint and missing value color
  # Also, specify that the legend should display dollars
  scale_fill_gradient2(low="blue", 
                       mid="white", 
                       high="red",
                       midpoint=mean(COUNTYnow$ZHVI,na.rm=TRUE),
                       na.value="gray",
                       labels=scales::dollar_format())
```

Then let's examine the annualized growth rates and regression residuals. We'll plot the maps for these two variables as well.

```{r countymaps}
# Plot the map
COUNTYmerge |>
  ggplot(aes(x=long, 
             y=lat, 
             group=group, 
             fill=AnnGrowthRollMean/100)) +
  # Generate empty outline to counties
  geom_polygon(color=NA) +
  # Perform Albers projection
  coord_map(projection="albers", 
            lat0=39,
            lat1=45) +
  # Label the fill color
  labs(fill="AnnGrowth", 
       title=paste("Average Annualized ZHVI Growth by State for 12 months ended ",as.character(COUNTYnow$Date[1]),sep="")) +
  # Create a color gradient for the fill color and specify the midpoint and missing value color
  # Also, specify that the legend should display dollars
  scale_fill_gradient2(low="blue", 
                       mid="white", 
                       high="red",
                       midpoint=0,
                       na.value="gray",
                       labels=scales::percent_format())

# Plot the map
COUNTYmerge |>
  ggplot(aes(x=long, 
             y=lat, 
             group=group, 
             fill=RegResidsRollMean/100)) +
  # Generate empty outline to counties
  geom_polygon(color=NA) +
  # Perform Albers projection
  coord_map(projection="albers", 
            lat0=39,
            lat1=45) +
  # Label the fill color
  labs(fill="RegResids", 
       title=paste("ZHVI Market Model Residuals by State for 12 months ended ",as.character(COUNTYnow$Date[1]),sep="")) +
  # Create a color gradient for the fill color and specify the midpoint and missing value color
  # Also, specify that the legend should display dollars
  scale_fill_gradient2(low="blue", 
                       mid="white", 
                       high="red",
                       midpoint=0,
                       na.value="gray",
                       labels=scales::percent_format())
```

### Mapping Missouri Counties

Since the U.S. map is a bit cluttered with all the counties, let's focus on Missouri. All we need to do here is to filter the merged county-level data to Missouri and then its all the same code as above.

```{r mocountymap}
# Plot the map
COUNTYmerge |>
  filter(state_name=="Missouri") |>
  ggplot(aes(x=long, 
             y=lat, 
             group=group, 
             fill=ZHVI)) +
  # Generate black outline to states
  geom_polygon(color=NA) +
  # Perform Albers projection
  coord_map(projection="albers", 
            lat0=39,
            lat1=45) +
  # Label the fill color
  labs(fill="ZHVI", 
       title=paste("ZHVI by Missouri County as of ",as.character(COUNTYnow$Date[1]),sep="")) +
  # Create a color gradient for the fill color and specify the midpoint and missing value color
  # Also, specify that the legend should display dollars
  scale_fill_gradient2(low="blue", 
                       mid="white", 
                       high="red",
                       midpoint=mean(COUNTYnow$ZHVI,na.rm=TRUE),
                       na.value="gray",
                       labels=scales::dollar_format())
```


## ZIP-code-Level ZHVI Data

Now to the ZIP-code-level data. With more than 27,000 ZIP codes, this produces a cleaned long data frame with more than 7.5 million observations.

```{r zipdata}
ZIPurl = paste(urlbase,"Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv", sep="")
ZIPraw = read_csv(ZIPurl, show_col_types=FALSE)
nrow(ZIPraw)
head(ZIPraw)
ZIPlong = pivot_longer(ZIPraw,
                          cols=10:ncol(ZIPraw),
                          names_to="Date",
                          values_to="ZHVI")
ZIPlong$Date = as.Date(ZIPlong$Date)
ZIPlong$RegionName = as.factor(ZIPlong$RegionName)
```

### ZIP-code-level Housing Returns

Now let's loop through each ZIP code to compute the housing returns. This loop will take a bit longer to run since there are over 27,000 ZIP codes to loop through. This one may take a few hours.

```{r ziploop}
# Preallocate column for annualized housing returns
ZIPlong$AnnGrowth = rep(NA,nrow(ZIPlong),1)
# Preallocate list to compile market-model regression results
LinModels_ZIP = vector(mode="list", length(levels(ZIPlong$RegionName)))
names(LinModels_ZIP) = levels(ZIPlong$RegionName)
# Preallocate column for regression residuals
ZIPlong$RegResids = rep(NA,nrow(ZIPlong),1)
# Preallocate columns for rolling averages
ZIPlong$ZHVIRollMean = rep(NA,nrow(ZIPlong),1)
ZIPlong$AnnGrowthRollMean = rep(NA,nrow(ZIPlong),1)
ZIPlong$RegResidsRollMean = rep(NA,nrow(ZIPlong),1)
# Set iteration counter and timer
i=1
t=proc.time()
# Loop through each zip
#zip = uniqueZIPIDs[1]
for (zip in levels(ZIPlong$RegionName)) {
  # Identify indices for the msa in full data frame
  zipidx = ZIPlong$RegionName==zip # & !is.na(ZIPlong$ZHVI)
  # Extract that subset
  zipdf = ZIPlong[zipidx,]
  # Impute missing observations (below uses linear interpolation if previously observed)
  if (sum(!is.na(zipdf$ZHVI))>2) {
    zipdf$ZHVI = approxfun(1:nrow(zipdf),zipdf$ZHVI)(1:nrow(zipdf))
  # Replace missing with imputed values in main data
  ZIPlong$ZHVI[zipidx] = zipdf$ZHVI
  # Convert to xts object
  zipxts = xts(zipdf$ZHVI,order.by=zipdf$Date)
  # Compute monthly log returns
  ziprets = log(as.numeric(zipxts)) - log(as.numeric(lag(zipxts)))
  # Convert to annualized percentages and save to full table
  ZIPlong$AnnGrowth[zipidx] = ziprets*12*100
  # Regress these MSA-level returns on the national returns
  LinModels_ZIP[[i]] = lm(ZIPlong$AnnGrowth[zipidx]~AnnGrowth,data=NATIONAL)
  # Add model R-squared to saved regression results list
  LinModels_ZIP[[i]]$r.squared = summary(LinModels_ZIP[[i]])$r.squared
  # Extract model residuals to full table
  ZIPlong$RegResids[zipidx & !is.na(ZIPlong$ZHVI)] = c(NA,LinModels_ZIP[[i]]$residuals)
  # Compute 12-month rolling means for smoother year-over-year values
  ZIPlong$ZHVIRollMean[zipidx] = rollmean(zipdf$ZHVI,12,fill=NA,align="right")
  ZIPlong$AnnGrowthRollMean[zipidx] = rollmean(METROSlong$AnnGrowth[zipidx],12,fill=NA,align="right")
  ZIPlong$RegResidsRollMean[zipidx] = rollmean(METROSlong$RegResids[zipidx],12,fill=NA,align="right")
  }
  # Increment counter
  i=i+1
}
proc.time()-t
```

Now that we have the regression residuals, let's output the top 6 and bottom 6 zip codes with the largest residuals for the most recent month:

```{r ziprankings1mo}
ZIPnow = ZIPlong[ZIPlong$Date==max(ZIPlong$Date),]
head(ZIPnow[order(-ZIPnow$RegResids),c(2,3,6,8,10,11,12,13)])
head(ZIPnow[order(ZIPnow$RegResids),c(2,3,6,8,10,11,12,13)])
```

Then for a longer-term comparison, let's examine the top 6 and bottom 6 of the residual rolling averages.

```{r ziprankings1yr}
head(ZIPnow[order(-ZIPnow$RegResidsRollMean),c(2,3,6,8,10,14,15,16)])
head(ZIPnow[order(ZIPnow$RegResidsRollMean),c(2,3,6,8,10,14,15,16)])
```


## Neighborhood-level ZHVI Data

Now to the neighborhood-level data. With more than 21,000 neighborhood, this produces a cleaned long data frame with more than 6.2 million observations.

```{r neighbdata}
NEIGHBurl = paste(urlbase,"Neighborhood_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv", sep="")
NEIGHBraw = read_csv(NEIGHBurl, show_col_types=FALSE)
nrow(NEIGHBraw)
head(NEIGHBraw)
NEIGHBlong = pivot_longer(NEIGHBraw,
                          cols=10:ncol(NEIGHBraw),
                          names_to="Date",
                          values_to="ZHVI")
NEIGHBlong$Date = as.Date(NEIGHBlong$Date)
# Use RegionID instead of RegionName since latter is not unique identifier
NEIGHBlong$RegionID = as.factor(NEIGHBlong$RegionID)
```

### Neighborhood-level Housing Returns

Now let's loop through each neighborhood to compute the housing returns. This loop will take a bit longer to run since there are over 21,000 neighborhoods to loop through. This one may also take a few hours.

```{r neighbloop}
# Preallocate column for annualized housing returns
NEIGHBlong$AnnGrowth = rep(NA,nrow(NEIGHBlong),1)
# Preallocate list to compile market-model regression results
LinModels_NEIGHB = vector(mode="list", length(levels(NEIGHBlong$RegionID)))
names(LinModels_NEIGHB) = levels(NEIGHBlong$RegionID)
# Preallocate column for regression residuals
NEIGHBlong$RegResids = rep(NA,nrow(NEIGHBlong),1)
# Preallocate columns for rolling averages
NEIGHBlong$ZHVIRollMean = rep(NA,nrow(NEIGHBlong),1)
NEIGHBlong$AnnGrowthRollMean = rep(NA,nrow(NEIGHBlong),1)
NEIGHBlong$RegResidsRollMean = rep(NA,nrow(NEIGHBlong),1)
# Set iteration counter and timer
i=1
t=proc.time()
# Loop through each neighborhood
#neighb = uniqueNEIGHBIDs[1]
for (neighb in levels(NEIGHBlong$RegionID)) {
  # Identify indices for the msa in full data frame
  neighbidx = NEIGHBlong$RegionID==neighb # & !is.na(NEIGHBlong$ZHVI)
  # Extract that subset
  neighbdf = NEIGHBlong[neighbidx,]
  # Impute missing observations (below uses linear interpolation if previously observed)
  if (sum(!is.na(neighbdf$ZHVI))>2) {
    neighbdf$ZHVI = approxfun(1:nrow(neighbdf),neighbdf$ZHVI)(1:nrow(neighbdf))
  # Replace missing with imputed values in main data
  NEIGHBlong$ZHVI[neighbidx] = neighbdf$ZHVI
  # Convert to xts object
  neighbxts = xts(neighbdf$ZHVI,order.by=neighbdf$Date)
  # Compute monthly log returns
  neighbrets = log(as.numeric(neighbxts)) - log(as.numeric(lag(neighbxts)))
  # Convert to annualized percentages and save to full table
  NEIGHBlong$AnnGrowth[neighbidx] = neighbrets*12*100
  # Regress these MSA-level returns on the national returns
  LinModels_NEIGHB[[i]] = lm(NEIGHBlong$AnnGrowth[neighbidx]~AnnGrowth,data=NATIONAL)
  # Add model R-squared to saved regression results list
  LinModels_NEIGHB[[i]]$r.squared = summary(LinModels_NEIGHB[[i]])$r.squared
  # Extract model residuals to full table
  NEIGHBlong$RegResids[neighbidx & !is.na(NEIGHBlong$ZHVI)] = c(NA,LinModels_NEIGHB[[i]]$residuals)
  # Compute 12-month rolling means for smoother year-over-year values
  NEIGHBlong$ZHVIRollMean[neighbidx] = rollmean(neighbdf$ZHVI,12,fill=NA,align="right")
  NEIGHBlong$AnnGrowthRollMean[neighbidx] = rollmean(NEIGHBlong$AnnGrowth[neighbidx],12,fill=NA,align="right")
  NEIGHBlong$RegResidsRollMean[neighbidx] = rollmean(NEIGHBlong$RegResids[neighbidx],12,fill=NA,align="right")
  }
  # Increment counter
  i=i+1
}
proc.time()-t
```

Now that we have the regression residuals, let's output the top 6 and bottom 6 counties with the largest residuals for the most recent month:

```{r neighbrankings1mo}
NEIGHBnow = NEIGHBlong[NEIGHBlong$Date==max(NEIGHBlong$Date),]
head(NEIGHBnow[order(-NEIGHBnow$RegResids),c(2,3,6,8,10,11,12,13)])
head(NEIGHBnow[order(NEIGHBnow$RegResids),c(2,3,6,8,10,11,12,13)])
```

Then for a longer-term comparison, let's examine the top 6 and bottom 6 of the residual rolling averages.

```{r neighbrankings1yr}
head(NEIGHBnow[order(-NEIGHBnow$RegResidsRollMean),c(2,3,6,8,10,14,15,16)])
head(NEIGHBnow[order(NEIGHBnow$RegResidsRollMean),c(2,3,6,8,10,14,15,16)])
```


## Save Final Workspace

Now that we've completed the analysis and filled our environment with lots of data and analysis, let's save the final workspace to a file. This will allow us to pick up where we left off without having to re-run all the code above.

```{r savedata}
save.image("finalworkspace.RData")
```

